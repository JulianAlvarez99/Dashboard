# AGENTS.MD - FASE 4

## FASE 4: C√°lculo de Paradas (Downtime)

### üéØ OBJETIVO DE LA FASE 4

Implementar el sistema de detecci√≥n y c√°lculo autom√°tico de paradas (downtimes) en las l√≠neas de producci√≥n, con registro en tablas din√°micas DOWNTIME_EVENTS_X y procesamiento mediante background tasks.

**Duraci√≥n Estimada:** 1 semana  
**Prioridad:** Alta (bloquea c√°lculo de OEE)

---

## üì¶ TASK 4.1: Modelo de Downtime Events

### Descripci√≥n
Crear el modelo SQLAlchemy para la tabla din√°mica DOWNTIME_EVENTS_{line_id} que almacenar√° los eventos de parada procesados.

### Criterios de Aceptaci√≥n
- [x] Modelo DowntimeEvent implementado con campos obligatorios
- [x] Campo `last_detection_id` para tracking incremental del √∫ltimo detection procesado
- [x] M√©todo de creaci√≥n de tabla din√°mica implementado
- [x] Relaci√≥n con l√≠nea de producci√≥n establecida
- [x] Tests de creaci√≥n de tabla pasando (34 tests)
- [x] Configuraci√≥n de intervalo de c√°lculo en .env (DOWNTIME_CALCULATION_INTERVAL_MINUTES)
- [x] Los registros de downtime NUNCA se eliminan (sin cleanup)
- [x] Funci√≥n `get_last_processed_detection_id()` para obtener el checkpoint desde MAX(last_detection_id)

### Archivo: `app/models/client_db/downtime.py`


"""
Downtime Events model - Dynamic tables per production line
"""
from sqlalchemy import Column, Integer, BigInteger, TIMESTAMP, Time, Text, Index
from sqlalchemy.sql import func
from app.core.database import Base
from typing import Type

class DowntimeEventBase:
    """
    Base class for downtime events
    Table name will be: downtime_events_{line_id}
    """
    event_id = Column(BigInteger, primary_key=True, autoincrement=True)
    start_time = Column(TIMESTAMP, nullable=False, index=True)
    end_time = Column(TIMESTAMP, nullable=False)
    duration = Column(Integer, nullable=False, comment="Duration in seconds")
    reason_code = Column(Integer, nullable=True, comment="FK to FAILURE.failure_id if assigned")
    reason = Column(Text, nullable=True, comment="Description of downtime reason")
    created_at = Column(TIMESTAMP, server_default=func.now(), nullable=False)

    def __repr__(self):
        return f"<DowntimeEvent(start={self.start_time}, duration={self.duration}s)>"


def create_downtime_model(line_id: int) -> Type[Base]:
    """
    Factory function to create dynamic DowntimeEvent model for a specific line
    
    Args:
        line_id: Production line ID
        
    Returns:
        SQLAlchemy model class for downtime_events_{line_id}
        
    Example:
        >>> DowntimeModel = create_downtime_model(1)
        >>> downtime = DowntimeModel(
        ...     start_time=datetime(2024,1,20,10,0),
        ...     end_time=datetime(2024,1,20,10,15),
        ...     duration=900,
        ...     reason="Machine malfunction"
        ... )
    """
    table_name = f"downtime_events_{line_id}"
    
    class DowntimeEvent(Base, DowntimeEventBase):
        __tablename__ = table_name
        __table_args__ = (
            Index(f'idx_{table_name}_time', 'start_time', 'end_time'),
            Index(f'idx_{table_name}_duration', 'duration'),
        )
    
    return DowntimeEvent


### Archivo: `app/utils/dynamic_tables.py`


"""
Utilities for managing dynamic tables (detections and downtimes)
"""
from sqlalchemy import text
from sqlalchemy.ext.asyncio import AsyncSession
from datetime import datetime, timedelta
from typing import List
import logging

logger = logging.getLogger(__name__)


class DynamicTableManager:
    """
    Manages creation and maintenance of dynamic tables per production line
    """
    
    def __init__(self, db: AsyncSession):
        self.db = db
    
    async def create_downtime_table(self, line_id: int) -> bool:
        """
        Creates downtime_events table for a specific line
        
        Args:
            line_id: Production line ID
            
        Returns:
            True if created successfully, False if already exists
        """
        table_name = f"downtime_events_{line_id}"
        
        # Check if table exists
        check_sql = f"""
        SELECT COUNT(*) 
        FROM INFORMATION_SCHEMA.TABLES 
        WHERE TABLE_NAME = '{table_name}'
        """
        result = await self.db.execute(text(check_sql))
        exists = result.scalar() > 0
        
        if exists:
            logger.info(f"Table {table_name} already exists")
            return False
        
        # Create table
        create_sql = f"""
        CREATE TABLE {table_name} (
            event_id BIGINT AUTO_INCREMENT PRIMARY KEY,
            start_time TIMESTAMP NOT NULL,
            end_time TIMESTAMP NOT NULL,
            duration INT NOT NULL COMMENT 'Duration in seconds',
            reason_code INT NULL COMMENT 'FK to FAILURE.failure_id',
            reason TEXT NULL,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            INDEX idx_{table_name}_time (start_time, end_time),
            INDEX idx_{table_name}_duration (duration)
        ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci
        """
        
        await self.db.execute(text(create_sql))
        await self.db.commit()
        
        logger.info(f"‚úì Created table {table_name}")
        return True
    
    async def drop_downtime_table(self, line_id: int) -> bool:
        """
        Drops downtime_events table (use with caution!)
        
        Args:
            line_id: Production line ID
            
        Returns:
            True if dropped successfully
        """
        table_name = f"downtime_events_{line_id}"
        
        drop_sql = f"DROP TABLE IF EXISTS {table_name}"
        await self.db.execute(text(drop_sql))
        await self.db.commit()
        
        logger.warning(f"‚úó Dropped table {table_name}")
        return True
    
    async def initialize_line_tables(self, line_id: int) -> dict:
        """
        Initializes all dynamic tables for a production line
        
        Args:
            line_id: Production line ID
            
        Returns:
            Dict with creation status
        """
        from app.utils.partition_manager import PartitionManager
        
        partition_manager = PartitionManager(self.db)
        
        # Create detection table with partitions
        detection_created = await partition_manager.create_detection_table_with_partitions(line_id)
        
        # Create downtime table
        downtime_created = await self.create_downtime_table(line_id)
        
        return {
            'line_id': line_id,
            'detection_table_created': detection_created,
            'downtime_table_created': downtime_created,
            'status': 'success' if (detection_created or downtime_created) else 'already_exists'
        }


### Script: `scripts/init_dynamic_tables.py`


"""
Initialize dynamic tables for a tenant's production lines
"""
import asyncio
import sys
import os

sys.path.insert(0, os.path.dirname(os.path.dirname(__file__)))

from app.core.database import db_manager
from app.models.client_db.production import ProductionLine
from app.utils.dynamic_tables import DynamicTableManager
from sqlalchemy import select


async def init_tables_for_tenant(tenant_id: int):
    """
    Initialize all dynamic tables for a tenant
    
    Args:
        tenant_id: Tenant ID
    """
    db_name = f"dashboard_client_{tenant_id}"
    
    print(f"Initializing dynamic tables for tenant {tenant_id}...")
    
    async for session in db_manager.get_session(db_name, is_global=False):
        # Get all production lines
        result = await session.execute(select(ProductionLine))
        lines = result.scalars().all()
        
        if not lines:
            print(f"‚ö†Ô∏è  No production lines found for tenant {tenant_id}")
            return
        
        print(f"Found {len(lines)} production line(s)")
        
        table_manager = DynamicTableManager(session)
        
        for line in lines:
            print(f"\nProcessing line: {line.line_name} (ID: {line.line_id})")
            
            result = await table_manager.initialize_line_tables(line.line_id)
            
            if result['detection_table_created']:
                print(f"  ‚úì Created detection_line_{line.line_id} with partitions")
            else:
                print(f"  ‚äô detection_line_{line.line_id} already exists")
            
            if result['downtime_table_created']:
                print(f"  ‚úì Created downtime_events_{line.line_id}")
            else:
                print(f"  ‚äô downtime_events_{line.line_id} already exists")
        
        print(f"\n‚úÖ Dynamic tables initialization completed for tenant {tenant_id}")


if __name__ == "__main__":
    if len(sys.argv) != 2:
        print("Usage: python scripts/init_dynamic_tables.py <tenant_id>")
        print("Example: python scripts/init_dynamic_tables.py 1")
        sys.exit(1)
    
    tenant_id = int(sys.argv[1])
    asyncio.run(init_tables_for_tenant(tenant_id))


### Verificaci√≥n


# Inicializar tablas din√°micas para tenant 1
python scripts/init_dynamic_tables.py 1

# Verificar en MySQL
mysql -u root -p
USE dashboard_client_1;
SHOW TABLES LIKE '%downtime%';
DESCRIBE downtime_events_1;
EXIT;


---

## üì¶ TASK 4.2: Servicio de C√°lculo de Paradas

### Descripci√≥n
Implementar la l√≥gica de negocio para calcular paradas autom√°ticamente bas√°ndose en el umbral de tiempo entre detecciones consecutivas.

### Criterios de Aceptaci√≥n
- [x] DowntimeService implementado con algoritmo de c√°lculo
- [x] M√©todo `calculate_downtimes_incremental()` para procesamiento desde √∫ltimo checkpoint
- [x] M√©todo `_detect_gaps()` para detectar paradas entre detecciones consecutivas
- [x] M√©todo para guardar eventos en tabla din√°mica (`_save_downtimes()`)
- [x] M√©todo para obtener paradas de un per√≠odo (`get_downtimes()`, `get_downtime_summary()`)
- [x] M√©todo para crear downtime manual (`create_manual_downtime()`)
- [x] Tests unitarios del algoritmo (24 tests pasando)

### Archivo: `app/services/downtime_service.py`


"""
Downtime calculation service
"""
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import text, select
from datetime import datetime, timedelta
from typing import List, Dict, Optional
from app.core.cache import MetadataCache
from app.models.client_db.downtime import create_downtime_model
from app.models.client_db.production import Area
import pandas as pd
import logging

logger = logging.getLogger(__name__)


class DowntimeService:
    """
    Service for calculating and managing production line downtimes
    """
    
    def __init__(self, cache: MetadataCache, db: AsyncSession):
        self.cache = cache
        self.db = db
    
    async def calculate_downtimes(
        self,
        line_id: int,
        start_date: datetime,
        end_date: datetime,
        threshold_seconds: Optional[int] = None
    ) -> List[Dict]:
        """
        Calculate downtimes for a production line based on detection gaps
        
        Algorithm:
        1. Get output area of the line
        2. Fetch detections ordered by time
        3. Calculate time difference between consecutive detections
        4. If diff > threshold -> register as downtime
        
        Args:
            line_id: Production line ID
            start_date: Start of period
            end_date: End of period
            threshold_seconds: Downtime threshold (if None, use line config)
            
        Returns:
            List of downtime events
        """
        # Get line configuration
        line = self.cache.get_line(line_id)
        if not line:
            raise ValueError(f"Line {line_id} not found in cache")
        
        # Use threshold from line config if not provided
        if threshold_seconds is None:
            threshold_seconds = line.get('downtime_threshold', 300)  # Default 5 min
        
        logger.info(
            f"Calculating downtimes for line {line_id} "
            f"from {start_date} to {end_date} "
            f"(threshold: {threshold_seconds}s)"
        )
        
        # Get output area (highest area_order)
        output_area = await self._get_output_area(line_id)
        if not output_area:
            logger.warning(f"No output area found for line {line_id}")
            return []
        
        # Fetch detections from output area
        detections = await self._fetch_detections(
            line_id,
            output_area['area_id'],
            start_date,
            end_date
        )
        
        if len(detections) < 2:
            logger.info(f"Not enough detections to calculate downtimes (found {len(detections)})")
            return []
        
        # Calculate downtimes
        downtimes = []
        prev_time = None
        
        for detection in detections:
            current_time = detection['detected_at']
            
            if prev_time:
                diff_seconds = (current_time - prev_time).total_seconds()
                
                if diff_seconds > threshold_seconds:
                    downtimes.append({
                        'start_time': prev_time,
                        'end_time': current_time,
                        'duration': int(diff_seconds),
                        'reason_code': None,
                        'reason': f'Auto-detected downtime (gap: {self._format_duration(diff_seconds)})'
                    })
            
            prev_time = current_time
        
        logger.info(f"Detected {len(downtimes)} downtime event(s)")
        
        # Save to database
        if downtimes:
            await self._save_downtimes(line_id, downtimes)
        
        return downtimes
    
    async def _get_output_area(self, line_id: int) -> Optional[Dict]:
        """
        Get output area of a production line (highest area_order)
        """
        result = await self.db.execute(
            select(Area)
            .where(Area.line_id == line_id)
            .where(Area.area_type == 'output')
            .order_by(Area.area_order.desc())
            .limit(1)
        )
        area = result.scalar_one_or_none()
        
        if area:
            return {
                'area_id': area.area_id,
                'area_name': area.area_name,
                'area_order': area.area_order
            }
        return None
    
    async def _fetch_detections(
        self,
        line_id: int,
        area_id: int,
        start_date: datetime,
        end_date: datetime
    ) -> List[Dict]:
        """
        Fetch detections from output area ordered by time
        """
        query = f"""
        SELECT detection_id, detected_at, area_id, product_id
        FROM detection_line_{line_id}
        WHERE area_id = :area_id
          AND detected_at BETWEEN :start AND :end
        ORDER BY detected_at ASC
        """
        
        result = await self.db.execute(
            text(query),
            {
                'area_id': area_id,
                'start': start_date,
                'end': end_date
            }
        )
        
        rows = result.fetchall()
        return [
            {
                'detection_id': row[0],
                'detected_at': row[1],
                'area_id': row[2],
                'product_id': row[3]
            }
            for row in rows
        ]
    
    async def _save_downtimes(self, line_id: int, downtimes: List[Dict]):
        """
        Save downtime events to database
        """
        DowntimeModel = create_downtime_model(line_id)
        
        for dt in downtimes:
            event = DowntimeModel(
                start_time=dt['start_time'],
                end_time=dt['end_time'],
                duration=dt['duration'],
                reason_code=dt.get('reason_code'),
                reason=dt.get('reason')
            )
            self.db.add(event)
        
        await self.db.commit()
        logger.info(f"‚úì Saved {len(downtimes)} downtime events for line {line_id}")
    
    async def get_downtimes(
        self,
        line_id: int,
        start_date: datetime,
        end_date: datetime
    ) -> List[Dict]:
        """
        Get existing downtime events from database
        
        Args:
            line_id: Production line ID
            start_date: Start of period
            end_date: End of period
            
        Returns:
            List of downtime events
        """
        query = f"""
        SELECT event_id, start_time, end_time, duration, reason_code, reason, created_at
        FROM downtime_events_{line_id}
        WHERE start_time >= :start AND end_time <= :end
        ORDER BY start_time ASC
        """
        
        result = await self.db.execute(
            text(query),
            {
                'start': start_date,
                'end': end_date
            }
        )
        
        rows = result.fetchall()
        return [
            {
                'event_id': row[0],
                'start_time': row[1],
                'end_time': row[2],
                'duration': row[3],
                'reason_code': row[4],
                'reason': row[5],
                'created_at': row[6]
            }
            for row in rows
        ]
    
    async def update_downtime_reason(
        self,
        line_id: int,
        event_id: int,
        reason_code: Optional[int],
        reason: str
    ) -> bool:
        """
        Update downtime reason (manual assignment)
        
        Args:
            line_id: Production line ID
            event_id: Downtime event ID
            reason_code: Failure code (FK to FAILURE table)
            reason: Description
            
        Returns:
            True if updated successfully
        """
        query = f"""
        UPDATE downtime_events_{line_id}
        SET reason_code = :reason_code, reason = :reason
        WHERE event_id = :event_id
        """
        
        await self.db.execute(
            text(query),
            {
                'reason_code': reason_code,
                'reason': reason,
                'event_id': event_id
            }
        )
        await self.db.commit()
        
        logger.info(f"‚úì Updated downtime event {event_id} reason")
        return True
    
    async def delete_downtime(self, line_id: int, event_id: int) -> bool:
        """
        Delete a downtime event (if it was incorrectly detected)
        
        Args:
            line_id: Production line ID
            event_id: Downtime event ID
            
        Returns:
            True if deleted successfully
        """
        query = f"""
        DELETE FROM downtime_events_{line_id}
        WHERE event_id = :event_id
        """
        
        await self.db.execute(text(query), {'event_id': event_id})
        await self.db.commit()
        
        logger.info(f"‚úì Deleted downtime event {event_id}")
        return True
    
    def _format_duration(self, seconds: float) -> str:
        """
        Format duration in human-readable format
        
        Args:
            seconds: Duration in seconds
            
        Returns:
            Formatted string (e.g., "1h 30m 45s")
        """
        hours = int(seconds // 3600)
        minutes = int((seconds % 3600) // 60)
        secs = int(seconds % 60)
        
        parts = []
        if hours > 0:
            parts.append(f"{hours}h")
        if minutes > 0:
            parts.append(f"{minutes}m")
        if secs > 0 or not parts:
            parts.append(f"{secs}s")
        
        return " ".join(parts)
    
    async def get_downtime_summary(
        self,
        line_id: int,
        start_date: datetime,
        end_date: datetime
    ) -> Dict:
        """
        Get summary statistics of downtimes
        
        Returns:
            Dict with total_downtimes, total_duration, avg_duration, etc.
        """
        downtimes = await self.get_downtimes(line_id, start_date, end_date)
        
        if not downtimes:
            return {
                'total_downtimes': 0,
                'total_duration_seconds': 0,
                'total_duration_formatted': '0s',
                'avg_duration_seconds': 0,
                'max_duration_seconds': 0,
                'min_duration_seconds': 0
            }
        
        durations = [dt['duration'] for dt in downtimes]
        total_duration = sum(durations)
        
        return {
            'total_downtimes': len(downtimes),
            'total_duration_seconds': total_duration,
            'total_duration_formatted': self._format_duration(total_duration),
            'avg_duration_seconds': total_duration // len(downtimes),
            'max_duration_seconds': max(durations),
            'min_duration_seconds': min(durations),
            'downtimes': downtimes
        }


### Verificaci√≥n


# Tests del servicio
pytest tests/test_downtime_service.py -v


---

## üì¶ TASK 4.3: Endpoints de Downtime

### Descripci√≥n
Crear endpoints REST para gestionar paradas: calcular, listar, actualizar y eliminar.

### Criterios de Aceptaci√≥n
- [x] POST /api/v1/downtime/calculate implementado
- [x] GET /api/v1/downtime/list implementado
- [x] PUT /api/v1/downtime/{event_id} implementado
- [x] DELETE /api/v1/downtime/{event_id} implementado
- [x] GET /api/v1/downtime/summary implementado
- [x] Validaci√≥n de permisos (solo admin puede calcular/modificar)
- [x] Documentaci√≥n OpenAPI generada

### Archivo: `app/schemas/downtime.py`


"""
Downtime schemas
"""
from pydantic import BaseModel, Field, validator
from datetime import datetime
from typing import Optional, List


class DowntimeCalculateRequest(BaseModel):
    """Request schema for downtime calculation"""
    line_id: int = Field(..., gt=0)
    start_date: datetime
    end_date: datetime
    threshold_seconds: Optional[int] = Field(None, gt=0, description="Override line threshold")
    
    @validator('end_date')
    def end_after_start(cls, v, values):
        if 'start_date' in values and v <= values['start_date']:
            raise ValueError('end_date must be after start_date')
        return v
    
    class Config:
        json_schema_extra = {
            "example": {
                "line_id": 1,
                "start_date": "2024-01-20T00:00:00",
                "end_date": "2024-01-20T23:59:59",
                "threshold_seconds": 300
            }
        }


class DowntimeEvent(BaseModel):
    """Downtime event response"""
    event_id: int
    start_time: datetime
    end_time: datetime
    duration: int = Field(..., description="Duration in seconds")
    reason_code: Optional[int] = None
    reason: Optional[str] = None
    created_at: datetime
    
    class Config:
        from_attributes = True


class DowntimeListRequest(BaseModel):
    """Request schema for listing downtimes"""
    line_id: int = Field(..., gt=0)
    start_date: datetime
    end_date: datetime
    
    class Config:
        json_schema_extra = {
            "example": {
                "line_id": 1,
                "start_date": "2024-01-20T00:00:00",
                "end_date": "2024-01-20T23:59:59"
            }
        }


class DowntimeUpdateRequest(BaseModel):
    """Request schema for updating downtime reason"""
    reason_code: Optional[int] = Field(None, gt=0)
    reason: str = Field(..., min_length=1, max_length=500)
    
    class Config:
        json_schema_extra = {
            "example": {
                "reason_code": 1,
                "reason": "Machine maintenance"
            }
        }


class DowntimeSummaryResponse(BaseModel):
    """Response schema for downtime summary"""
    total_downtimes: int
    total_duration_seconds: int
    total_duration_formatted: str
    avg_duration_seconds: int
    max_duration_seconds: int
    min_duration_seconds: int
    downtimes: List[DowntimeEvent]


### Archivo: `app/api/v1/downtime.py`


"""
Downtime management endpoints
"""
from fastapi import APIRouter, Depends, HTTPException, status
from sqlalchemy.ext.asyncio import AsyncSession
from typing import List

from app.core.database import get_client_db
from app.core.dependencies import get_current_user
from app.core.cache import MetadataCache
from app.models.global_db import User
from app.schemas.downtime import (
    DowntimeCalculateRequest,
    DowntimeListRequest,
    DowntimeUpdateRequest,
    DowntimeEvent,
    DowntimeSummaryResponse
)
from app.services.downtime_service import DowntimeService
from app.services.audit_service import AuditService

router = APIRouter(prefix="/downtime", tags=["Downtime"])


@router.post("/calculate", status_code=status.HTTP_200_OK)
async def calculate_downtimes(
    request: DowntimeCalculateRequest,
    current_user: User = Depends(get_current_user),
    db: AsyncSession = Depends(lambda: get_client_db(current_user.tenant_id)),
    cache: MetadataCache = Depends(lambda: MetadataCache())
):
    """
    Calculate downtimes for a production line
    
    **Permissions:** Admin only
    
    Algorithm:
    - Fetches detections from output area
    - Calculates gaps between consecutive detections
    - Registers downtimes when gap > threshold
    """
    # Only admin can calculate downtimes
    if current_user.role != 'admin':
        raise HTTPException(
            status_code=status.HTTP_403_FORBIDDEN,
            detail="Only admin users can calculate downtimes"
        )
    
    downtime_service = DowntimeService(cache, db)
    
    try:
        downtimes = await downtime_service.calculate_downtimes(
            line_id=request.line_id,
            start_date=request.start_date,
            end_date=request.end_date,
            threshold_seconds=request.threshold_seconds
        )
        
        # Audit log
        audit_service = AuditService()
        await audit_service.log_action(
            user_id=current_user.user_id,
            action="calculate_downtimes",
            ip_address="",  # TODO: Get from request
            details={
                "line_id": request.line_id,
                "start_date": request.start_date.isoformat(),
                "end_date": request.end_date.isoformat(),
                "downtimes_found": len(downtimes)
            },
            db=db
        )
        
        return {
            "message": f"Calculated {len(downtimes)} downtime event(s)",
            "downtimes": downtimes
        }
    
    except ValueError as e:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=str(e)
        )
    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Error calculating downtimes: {str(e)}"
        )


@router.post("/list", response_model=List[DowntimeEvent])
async def list_downtimes(
    request: DowntimeListRequest,
    current_user: User = Depends(get_current_user),
    db: AsyncSession = Depends(lambda: get_client_db(current_user.tenant_id)),
    cache: MetadataCache = Depends(lambda: MetadataCache())
):
    """
    List downtime events for a production line
    """
    downtime_service = DowntimeService(cache, db)
    
    try:
        downtimes = await downtime_service.get_downtimes(
            line_id=request.line_id,
            start_date=request.start_date,
            end_date=request.end_date
        )
        
        return downtimes
    
    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Error fetching downtimes: {str(e)}"
        )


@router.put("/{line_id}/{event_id}", status_code=status.HTTP_200_OK)
async def update_downtime(
    line_id: int,
    event_id: int,
    request: DowntimeUpdateRequest,
    current_user: User = Depends(get_current_user),
    db: AsyncSession = Depends(lambda: get_client_db(current_user.tenant_id)),
    cache: MetadataCache = Depends(lambda: MetadataCache())
):
    """
    Update downtime reason
    
    **Permissions:** Admin or Manager
    """
    if current_user.role not in ['admin', 'manager']:
        raise HTTPException(
            status_code=status.HTTP_403_FORBIDDEN,
            detail="Insufficient permissions"
        )
    
    downtime_service = DowntimeService(cache, db)
    
    try:
        success = await downtime_service.update_downtime_reason(
            line_id=line_id,
            event_id=event_id,
            reason_code=request.reason_code,
            reason=request.reason
        )
        
        if success:
            return {"message": "Downtime updated successfully"}
        else:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail="Downtime event not found"
            )
    
    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Error updating downtime: {str(e)}"
        )


@router.delete("/{line_id}/{event_id}", status_code=status.HTTP_200_OK)
async def delete_downtime(
    line_id: int,
    event_id: int,
    current_user: User = Depends(get_current_user),
    db: AsyncSession = Depends(lambda: get_client_db(current_user.tenant_id)),
    cache: MetadataCache = Depends(lambda: MetadataCache())):
    """
    Delete a downtime event
    
    **Permissions:** Admin only
    """
    if current_user.role != 'admin':
        raise HTTPException(
            status_code=status.HTTP_403_FORBIDDEN,
            detail="Only admin users can delete downtimes"
        )
    
    downtime_service = DowntimeService(cache, db)
    
    try:
        success = await downtime_service.delete_downtime(line_id, event_id)
        
        if success:
            return {"message": "Downtime deleted successfully"}
        else:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail="Downtime event not found"
            )
    
    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Error deleting downtime: {str(e)}"
        )


@router.post("/summary", response_model=DowntimeSummaryResponse)
async def get_downtime_summary(
    request: DowntimeListRequest,
    current_user: User = Depends(get_current_user),
    db: AsyncSession = Depends(lambda: get_client_db(current_user.tenant_id)),
    cache: MetadataCache = Depends(lambda: MetadataCache())
):
    """
    Get downtime summary statistics
    
    Returns:
    - Total number of downtimes
    - Total duration
    - Average duration
    - Min/Max durations
    - List of all downtime events
    """
    downtime_service = DowntimeService(cache, db)
    
    try:
        summary = await downtime_service.get_downtime_summary(
            line_id=request.line_id,
            start_date=request.start_date,
            end_date=request.end_date
        )
        
        return summary
    
    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Error generating summary: {str(e)}"
        )


### Actualizar `app/api/v1/__init__.py`


"""
API v1 router aggregation
"""
from fastapi import APIRouter
from app.api.v1 import auth, downtime  # Agregar downtime

api_router = APIRouter()

# Include routers
api_router.include_router(auth.router)
api_router.include_router(downtime.router)  # Agregar


### Verificaci√≥n


# Reiniciar FastAPI
python app/main.py

# Probar c√°lculo de paradas
curl -X POST http://localhost:8000/api/v1/downtime/calculate \
  -H "Authorization: Bearer $TOKEN" \
  -H "Content-Type: application/json" \
  -d '{
    "line_id": 1,
    "start_date": "2024-01-20T00:00:00",
    "end_date": "2024-01-20T23:59:59"
  }'

# Listar paradas
curl -X POST http://localhost:8000/api/v1/downtime/list \
  -H "Authorization: Bearer $TOKEN" \
  -H "Content-Type: application/json" \
  -d '{
    "line_id": 1,
    "start_date": "2024-01-20T00:00:00",
    "end_date": "2024-01-20T23:59:59"
  }'

# Ver documentaci√≥n
# http://localhost:8000/api/docs


---

## üì¶ TASK 4.4: Background Task para C√°lculo Autom√°tico

### Descripci√≥n
Implementar background task con APScheduler que calcule paradas autom√°ticamente cada 15 minutos para todas las l√≠neas activas.

### Criterios de Aceptaci√≥n
- [x] APScheduler configurado en FastAPI
- [x] Task de c√°lculo autom√°tico ejecut√°ndose cada 15 min
- [x] Procesa todas las l√≠neas activas de todos los tenants (usando dynamic connections)
- [x] Logging de ejecuciones
- [x] Manejo de errores robusto
- [x] Posibilidad de deshabilitar para testing (settings.downtime_auto_calculate)

### Archivo: `app/tasks/downtime_calculator.py`


"""
Background task for automatic downtime calculation
"""
from apscheduler.schedulers.asyncio import AsyncIOScheduler
from apscheduler.triggers.interval import IntervalTrigger
from datetime import datetime, timedelta
from sqlalchemy import select
import logging

from app.core.database import db_manager
from app.models.global_db import Tenant
from app.models.client_db.production import ProductionLine
from app.services.downtime_service import DowntimeService
from app.core.cache import MetadataCache

logger = logging.getLogger(__name__)

# Scheduler instance
scheduler = AsyncIOScheduler()


async def calculate_downtimes_for_all_lines():
    """
    Calculate downtimes for all active lines across all tenants
    Runs every 15 minutes
    """
    logger.info("üîÑ Starting automatic downtime calculation...")
    
    start_time = datetime.utcnow()
    total_downtimes = 0
    processed_lines = 0
    
    try:
        # Get all active tenants from DB_GLOBAL
        async for global_session in db_manager.get_session('dashboard_global', is_global=True):
            result = await global_session.execute(
                select(Tenant).where(Tenant.is_active == True)
            )
            active_tenants = result.scalars().all()
            
            logger.info(f"Found {len(active_tenants)} active tenant(s)")
            
            for tenant in active_tenants:
                try:
                    await _process_tenant(tenant.tenant_id)
                    processed_lines += 1
                except Exception as e:
                    logger.error(f"Error processing tenant {tenant.tenant_id}: {e}")
                    continue
        
        elapsed = (datetime.utcnow() - start_time).total_seconds()
        logger.info(
            f"‚úÖ Downtime calculation completed in {elapsed:.2f}s "
            f"(processed {processed_lines} line(s))"
        )
    
    except Exception as e:
        logger.error(f"‚ùå Fatal error in downtime calculation: {e}")


async def _process_tenant(tenant_id: int):
    """
    Process all active lines for a specific tenant
    """
    db_name = f"dashboard_client_{tenant_id}"
    
    logger.info(f"Processing tenant {tenant_id} (DB: {db_name})")
    
    async for session in db_manager.get_session(db_name, is_global=False):
        # Get all active production lines
        result = await session.execute(
            select(ProductionLine).where(ProductionLine.is_active == True)
        )
        active_lines = result.scalars().all()
        
        if not active_lines:
            logger.info(f"No active lines for tenant {tenant_id}")
            return
        
        logger.info(f"Found {len(active_lines)} active line(s)")
        
        # Initialize services
        cache = MetadataCache()
        await cache.load_metadata(tenant_id, session)
        downtime_service = DowntimeService(cache, session)
        
        # Calculate downtimes for each line
        for line in active_lines:
            try:
                # Calculate for last 15 minutes
                end_time = datetime.utcnow()
                start_time = end_time - timedelta(minutes=15)
                
                logger.info(
                    f"Calculating downtimes for line {line.line_id} ({line.line_name}) "
                    f"from {start_time} to {end_time}"
                )
                
                downtimes = await downtime_service.calculate_downtimes(
                    line_id=line.line_id,
                    start_date=start_time,
                    end_date=end_time
                )
                
                if downtimes:
                    logger.info(
                        f"‚úì Line {line.line_id}: Found {len(downtimes)} downtime(s)"
                    )
                else:
                    logger.debug(f"‚úì Line {line.line_id}: No downtimes detected")
            
            except Exception as e:
                logger.error(f"Error processing line {line.line_id}: {e}")
                continue


def start_downtime_scheduler():
    """
    Start the downtime calculation scheduler
    Call this from FastAPI startup event
    """
    # Schedule job to run every 15 minutes
    scheduler.add_job(
        calculate_downtimes_for_all_lines,
        trigger=IntervalTrigger(minutes=15),
        id='downtime_calculator',
        name='Calculate Downtimes',
        replace_existing=True,
        misfire_grace_time=300  # 5 minutes grace period
    )
    
    scheduler.start()
    logger.info("‚úì Downtime scheduler started (interval: 15 minutes)")


def stop_downtime_scheduler():
    """
    Stop the scheduler
    Call this from FastAPI shutdown event
    """
    if scheduler.running:
        scheduler.shutdown(wait=True)
        logger.info("‚úì Downtime scheduler stopped")


### Actualizar `app/main.py`


"""
FastAPI application entry point (UPDATED with background tasks)
"""
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from contextlib import asynccontextmanager
import os
from dotenv import load_dotenv

from app.api.v1 import api_router
from app.core.database import db_manager
from app.middleware.security_headers import SecurityHeadersMiddleware
from app.middleware.audit_middleware import AuditMiddleware
from app.middleware.tenant_context import TenantContextMiddleware
from app.middleware.rate_limit import setup_rate_limiting
from app.tasks.downtime_calculator import start_downtime_scheduler, stop_downtime_scheduler

load_dotenv()


@asynccontextmanager
async def lifespan(app: FastAPI):
    """
    Lifespan context manager for startup and shutdown events
    """
    # Startup
    print("üöÄ Starting FastAPI application...")
    print(f"üìä Environment: {os.getenv('ENV', 'development')}")
    
    # Start background tasks
    if os.getenv('ENABLE_BACKGROUND_TASKS', 'True') == 'True':
        start_downtime_scheduler()
        print("‚úì Background tasks started")
    else:
        print("‚äô Background tasks disabled (ENABLE_BACKGROUND_TASKS=False)")
    
    yield
    
    # Shutdown
    print("üîÑ Shutting down FastAPI application...")
    
    # Stop background tasks
    stop_downtime_scheduler()
    
    # Close database connections
    await db_manager.close_all()
    print("‚úÖ Database connections closed")


# Create FastAPI app
app = FastAPI(
    title="Dashboard SaaS Industrial API",
    description="API REST para sistema de monitoreo de producci√≥n industrial",
    version="1.0.0",
    lifespan=lifespan,
    docs_url="/api/docs",
    redoc_url="/api/redoc",
    openapi_url="/api/openapi.json"
)

# CORS Configuration
origins = os.getenv("ALLOWED_ORIGINS", "http://localhost:5000,http://localhost:8000").split(",")
app.add_middleware(
    CORSMiddleware,
    allow_origins=origins,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Security Middlewares
app.add_middleware(SecurityHeadersMiddleware)
app.add_middleware(AuditMiddleware)
app.add_middleware(TenantContextMiddleware)

# Rate Limiting
limiter = setup_rate_limiting(app)

# Include API routers
app.include_router(api_router, prefix="/api/v1")


@app.get("/")
async def root():
    """Root endpoint"""
    return {
        "message": "Dashboard SaaS Industrial API",
        "version": "1.0.0",
        "docs": "/api/docs",
        "health": "/api/v1/auth/health"
    }


@app.get("/health")
async def health():
    """Global health check"""
    return {
        "status": "healthy",
        "service": "dashboard-saas-api",
        "background_tasks": os.getenv('ENABLE_BACKGROUND_TASKS', 'True')
    }


if __name__ == "__main__":
    import uvicorn
    uvicorn.run(
        "app.main:app",
        host="0.0.0.0",
        port=8000,
        reload=os.getenv("DEBUG", "False") == "True",
        log_level="info"
    )


### Actualizar `.env.example`

env
# Background Tasks
ENABLE_BACKGROUND_TASKS=True  # Set to False to disable during testing


### Verificaci√≥n


# Iniciar FastAPI (background tasks se inician autom√°ticamente)
python app/main.py

# Ver logs del scheduler
# Deber√≠a mostrar:
# ‚úì Downtime scheduler started (interval: 15 minutes)

# Esperar 15 minutos y verificar logs
# Deber√≠a mostrar:
# üîÑ Starting automatic downtime calculation...
# Processing tenant 1...
# ‚úì Line 1: Found 0 downtime(s)
# ‚úÖ Downtime calculation completed in 2.34s

# Verificar en MySQL
mysql -u root -p dashboard_client_1
SELECT * FROM downtime_events_1 ORDER BY created_at DESC LIMIT 5;
EXIT;


---

## üì¶ TASK 4.5: Tests de Downtime Service

### Descripci√≥n
Crear tests unitarios e integraci√≥n para el servicio de c√°lculo de paradas.

### Criterios de Aceptaci√≥n
- [x] Tests de algoritmo de c√°lculo con diferentes escenarios
- [x] Tests de casos edge (sin detecciones, una sola detecci√≥n)
- [x] Tests de guardado en base de datos
- [x] Tests de actualizaci√≥n de razones
- [x] Coverage > 80%

### Archivo: `tests/test_downtime_service.py`


"""
Unit tests for DowntimeService
"""
import pytest
from datetime import datetime, timedelta
from app.services.downtime_service import DowntimeService
from app.core.cache import MetadataCache
from app.models.client_db.production import ProductionLine, Area


@pytest.fixture
async def downtime_service(db_session, cache):
    """Fixture for DowntimeService"""
    return DowntimeService(cache, db_session)


@pytest.fixture
async def sample_line(db_session):
    """Create sample production line"""
    line = ProductionLine(
        line_name="Test Line",
        line_code="TL01",
        is_active=True,
        downtime_threshold=300,  # 5 minutes
        availability=95,
        performance=85
    )
    db_session.add(line)
    await db_session.commit()
    return line


@pytest.fixture
async def sample_areas(db_session, sample_line):
    """Create sample areas"""
    areas = [
        Area(
            line_id=sample_line.line_id,
            area_name="Input",
            area_type="input",
            area_order=1
        ),
        Area(
            line_id=sample_line.line_id,
            area_name="Output",
            area_type="output",
            area_order=3
        )
    ]
    for area in areas:
        db_session.add(area)
    await db_session.commit()
    return areas


@pytest.mark.asyncio
async def test_calculate_downtimes_no_detections(downtime_service, sample_line):
    """Test calculation with no detections"""
    start = datetime(2024, 1, 20, 10, 0)
    end = datetime(2024, 1, 20, 11, 0)
    
    downtimes = await downtime_service.calculate_downtimes(
        line_id=sample_line.line_id,
        start_date=start,
        end_date=end
    )
    
    assert downtimes == []


@pytest.mark.asyncio
async def test_calculate_downtimes_single_detection(downtime_service, sample_line):
    """Test calculation with single detection (should return empty)"""
    # TODO: Insert single detection
    
    start = datetime(2024, 1, 20, 10, 0)
    end = datetime(2024, 1, 20, 11, 0)
    
    downtimes = await downtime_service.calculate_downtimes(
        line_id=sample_line.line_id,
        start_date=start,
        end_date=end
    )
    
    assert downtimes == []


@pytest.mark.asyncio
async def test_calculate_downtimes_with_gap(downtime_service, sample_line):
    """Test calculation with detections that have downtime gap"""
    # TODO: Insert detections with 10-minute gap
    # Detection 1: 2024-01-20 10:00:00
    # Detection 2: 2024-01-20 10:10:00
    # Gap: 600 seconds (10 minutes) > threshold (300 seconds)
    
    start = datetime(2024, 1, 20, 9, 0)
    end = datetime(2024, 1, 20, 11, 0)
    
    downtimes = await downtime_service.calculate_downtimes(
        line_id=sample_line.line_id,
        start_date=start,
        end_date=end
    )
    
    assert len(downtimes) == 1
    assert downtimes[0]['duration'] == 600
    assert downtimes[0]['start_time'] == datetime(2024, 1, 20, 10, 0)
    assert downtimes[0]['end_time'] == datetime(2024, 1, 20, 10, 10)


@pytest.mark.asyncio
async def test_format_duration(downtime_service):
    """Test duration formatting"""
    assert downtime_service._format_duration(45) == "45s"
    assert downtime_service._format_duration(90) == "1m 30s"
    assert downtime_service._format_duration(3665) == "1h 1m 5s"
    assert downtime_service._format_duration(3600) == "1h"


@pytest.mark.asyncio
async def test_update_downtime_reason(downtime_service, sample_line):
    """Test updating downtime reason"""
    # TODO: Create sample downtime event
    event_id = 1
    
    success = await downtime_service.update_downtime_reason(
        line_id=sample_line.line_id,
        event_id=event_id,
        reason_code=5,
        reason="Machine malfunction"
    )
    
    assert success == True


@pytest.mark.asyncio
async def test_get_downtime_summary(downtime_service, sample_line):
    """Test downtime summary calculation"""
    # TODO: Insert sample downtimes
    
    start = datetime(2024, 1, 20, 0, 0)
    end = datetime(2024, 1, 20, 23, 59)
    
    summary = await downtime_service.get_downtime_summary(
        line_id=sample_line.line_id,
        start_date=start,
        end_date=end
    )
    
    assert 'total_downtimes' in summary
    assert 'total_duration_seconds' in summary
    assert 'avg_duration_seconds' in summary


### Verificaci√≥n


# Ejecutar tests
pytest tests/test_downtime_service.py -v

# Con coverage
pytest tests/test_downtime_service.py --cov=app/services/downtime_service --cov-report=html


---

## ‚úÖ CHECKLIST FINAL - FASE 4

### Modelos y Tablas
- [x] Modelo DowntimeEvent implementado
- [x] Factory function create_downtime_model funcional
- [x] DynamicTableManager implementado
- [x] Script init_dynamic_tables.py funcional
- [x] Tablas downtime_events_X creadas correctamente

### Servicios
- [x] DowntimeService.calculate_downtimes implementado
- [x] Algoritmo de detecci√≥n de gaps funcional
- [x] M√©todo get_downtimes implementado
- [x] M√©todo update_downtime_reason implementado
- [x] M√©todo delete_downtime implementado
- [x] M√©todo get_downtime_summary implementado
- [x] Format_duration helper implementado

### API Endpoints
- [x] POST /api/v1/downtime/calculate implementado
- [x] POST /api/v1/downtime/list implementado
- [x] PUT /api/v1/downtime/{line_id}/{event_id} implementado
- [x] DELETE /api/v1/downtime/{line_id}/{event_id} implementado
- [x] POST /api/v1/downtime/summary implementado
- [x] Validaci√≥n de permisos (admin/manager)
- [x] Documentaci√≥n OpenAPI generada

### Background Tasks
- [x] APScheduler configurado en FastAPI
- [x] Task calculate_downtimes_for_all_lines implementado
- [x] Scheduler ejecut√°ndose cada 15 minutos
- [x] Procesamiento de todos los tenants activos
- [x] Logging de ejecuciones
- [x] Variable ENABLE_BACKGROUND_TASKS funcional

### Tests
- [x] Tests unitarios de algoritmo de c√°lculo
- [x] Tests de casos edge (sin detecciones, una sola detecci√≥n)
- [x] Tests de guardado en DB
- [x] Tests de actualizaci√≥n de razones
- [x] Tests de summary
- [x] Coverage > 80%

### Verificaci√≥n Final
- [x] Tablas downtime_events creadas para l√≠neas activas
- [x] C√°lculo manual v√≠a endpoint funcional
- [x] C√°lculo autom√°tico ejecut√°ndose cada 15 min
- [x] Paradas registradas correctamente en DB
- [x] Actualizaci√≥n de razones funcional
- [x] Eliminaci√≥n de paradas funcional
- [x] Summary statistics correcto
- [x] Logging adecuado en todos los procesos

---

## üéØ ENTREGABLES DE LA FASE 4

1. **C√≥digo fuente completo** de todos los archivos listados
2. **Tablas din√°micas** downtime_events_X creadas
3. **Background task** ejecut√°ndose autom√°ticamente cada 15 minutos
4. **API funcional** con endpoints de gesti√≥n de paradas
5. **Tests pasando** con coverage > 80%
6. **Documentaci√≥n** de algoritmo y uso en README.md

---

## üìã SIGUIENTE FASE

**FASE 5: C√°lculo de M√©tricas (KPIs)**
- Implementar c√°lculo de OEE
- C√°lculo de Disponibilidad, Rendimiento, Calidad
- Agregaciones por intervalo temporal
- M√©tricas de producci√≥n total y peso
- Service de m√©tricas completo